@article{carpenter2017,
  title = {Stan: {{A Probabilistic Programming Language}}},
  shorttitle = {Stan},
  author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus A. and Guo, Jiqiang and Li, Peter and Riddell, Allen},
  year = {2017},
  journal = {Journal of Statistical Software},
  volume = {76},
  pages = {1},
  issn = {1548-7660},
  doi = {10.18637/jss.v076.i01},
  urldate = {2024-10-10},
  abstract = {Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.14.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm., Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propagation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible., Stan can be called from the command line using the cmdstan package, through R using the rstan package, and through Python using the pystan package. All three interfaces support sampling and optimization-based inference with diagnostics and posterior analysis. rstan and pystan also provide access to log probabilities, gradients, Hessians, parameter transforms, and specialized plotting.},
  pmcid = {PMC9788645},
  pmid = {36568334},
  file = {/Users/s447341/Zotero/storage/BERXNNZ7/Carpenter et al. - 2017 - Stan A Probabilistic Programming Language.pdf}
}

@techreport{gabry2024,
  title = {{{cmdstanr}}: {{R Interface}} to '{{CmdStan}}'},
  author = {Gabry, Jonah and {\v C}e{\v s}novar, Rok and Johnson, Andrew and Bronder, Steve},
  year = {2024}
}

@article{glennie2023,
  title = {Hidden {{Markov}} Models: {{Pitfalls}} and Opportunities in Ecology},
  shorttitle = {Hidden {{Markov}} Models},
  author = {Glennie, Richard and Adam, Timo and {Leos-Barajas}, Vianey and Michelot, Th{\'e}o and Photopoulou, Theoni and McClintock, Brett T.},
  year = {2023},
  journal = {Methods in Ecology and Evolution},
  volume = {14},
  number = {1},
  pages = {43--56},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.13801},
  urldate = {2024-10-10},
  abstract = {Hidden Markov models (HMMs) and their extensions are attractive methods for analysing ecological data where noisy, multivariate measurements are made of a hidden, ecological process, and where this hidden process is represented by a sequence of discrete states. Yet, as these models become more complex and challenging to understand, it is important to consider what pitfalls these methods have and what opportunities there are for future research to address these pitfalls. In this paper, we review five lesser known pitfalls one can encounter when using HMMs or their extensions to solve ecological problems: (a) violation of the snapshot property in continuous-time HMMs; (b) biased inference from hierarchical HMMs when applied to temporally misaligned processes; (c) sensitive inference from using random effects to partially pool across heterogeneous individuals; (d) computational burden when using HMMs to approximate models with continuous state spaces; and (e) difficulty linking the hidden process to space or environment. This review is for ecologists and ecological statisticians familiar with HMMs, but who may be less aware of the problems that arise in more specialised applications. We demonstrate how each pitfall arises, by simulation or example, and discuss why this pitfall is important to consider. Along with identifying the problems, we highlight potential research opportunities and offer ideas that may help alleviate these pitfalls. Each of the methods we review are solutions to current ecological research problems. We intend for this paper to heighten awareness of the pitfalls ecologists may encounter when applying these more advanced methods, but we also hope that by highlighting future research opportunities, we can inspire ecological statisticians to weaken these pitfalls and provide improved methods.},
  langid = {english},
  keywords = {animal movement,continuous time,hidden Markov model,hierarchical model,population ecology,random effects,state space models,time series},
  file = {/Users/s447341/Zotero/storage/7Y8Z9UX6/Glennie et al. - 2023 - Hidden Markov models Pitfalls and opportunities in ecology.pdf}
}

@techreport{kay2024,
  title = {{{tidybayes}}: {{Tidy}} Data and Geoms for {{Bayesian}} Models},
  author = {Kay, Matthew},
  year = {2024},
  doi = {10.5281/zenodo.1308151}
}

@book{mcelreath2020,
  title = {Statistical Rethinking: A {{Bayesian}} Course with Examples in {{R}} and {{Stan}}},
  shorttitle = {Statistical Rethinking},
  author = {McElreath, Richard},
  year = {2020},
  series = {Chapman \& {{Hall}}/{{CRC}} Texts in Statistical Science Series},
  edition = {Second edition},
  publisher = {CRC Press},
  address = {Boca Raton},
  abstract = {"Statistical Rethinking: A Bayesian Course with Examples in R and Stan builds your knowledge of and confidence in making inferences from data. Reflecting the need for scripting in today's model-based statistics, the book pushes you to perform step-by-step calculations that are usually automated. This unique computational approach ensures that you understand enough of the details to make reasonable choices and interpretations in your own modeling work.The text presents causal inference and generalized linear multilevel models from a simple Bayesian perspective that builds on information theory and maximum entropy. The core material ranges from the basics of regression to advanced multilevel models. It also presents measurement error, missing data, and Gaussian process models for spatial and phylogenetic confounding.The second edition emphasizes the directed acyclic graph (DAG) approach to causal inference, integrating DAGs into many examples. The new edition also contains new material on the design of prior distributions, splines, ordered categorical predictors, social relations models, cross-validation, importance sampling, instrumental variables, and Hamiltonian Monte Carlo. It ends with an entirely new chapter that goes beyond generalized linear modeling, showing how domain-specific scientific models can be built into statistical analyses."--},
  isbn = {978-0-429-02960-8 978-0-429-63914-2},
  langid = {english},
  lccn = {Electronic Resource},
  keywords = {Bayes Theorem,Bayes-Entscheidungstheorie,Bayesian statistical decision theory,Computer programs,Computer software,Data Interpretation Statistical,Logiciels,Mathematical Computing,Mathematics,R,R (Computer program language),R (Langage de programmation),software,Software,Statistisches Modell,Theoreme de Bayes,Theorie de la decision bayesienne},
  file = {/Users/s447341/Zotero/storage/MAYU72EC/McElreath - 2020 - Statistical rethinking a Bayesian course with examples in R and Stan.pdf}
}
